version: '3'


#################################
##          Zookeeper          ##
#################################


x-zookeeper-variables:
  common: &zookeeper-common
    image: zookeeper:3.8.3
    restart: on-failure
    healthcheck:
      test: [ "CMD", "zkServer.sh", "status" ]
      interval: 10s
      retries: 5
  environment: &zookeeper-environment # https://hub.docker.com/_/zookeeper
    ZOO_PORT: 2181
    ZOO_SERVERS: >
      server.1=zk-1:2888:3888;2181
      server.2=zk-2:2888:3888;2181
      server.3=zk-3:2888:3888;2181
    ZOO_INIT_LIMIT: 10
    ZOO_SYNC_LIMIT: 5
    ZOO_TICK_TIME: 2000
    ZOO_DATA_DIR: /data

x-zookeeper-cluster: &zookeeper-cluster
  zk-1:
    <<: *zookeeper-common
    container_name: zk-1
    hostname: zk-1
    environment:
      ZOO_MY_ID: 1
      <<: *zookeeper-environment

  zk-2:
    <<: *zookeeper-common
    container_name: zk-2
    hostname: zk-2
    environment:
      ZOO_MY_ID: 2
      <<: *zookeeper-environment

  zk-3:
    <<: *zookeeper-common
    container_name: zk-3
    hostname: zk-3
    environment:
      ZOO_MY_ID: 3
      <<: *zookeeper-environment


####################################
##          Hadoop-Spark          ##
####################################


x-hadoop-spark-variables:
  common: &hadoop-spark-common
    image: data-infra:hadoop-spark
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        OPTIMIZE_HADOOP: true
        BUILD_SPARK: true
  master: &hadoop-spark-master
    depends_on:
      - zk-1
      - zk-2
      - zk-3
      - hdp-worker-1
      - hdp-worker-2
      - hdp-worker-3
  worker: &hadoop-spark-worker
    command: ["init-hadoop-spark-worker.sh"]

x-hadoop-spark-cluster: &hadoop-spark-cluster
  hdp-master-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-1
    hostname: hdp-master-1
    command: ["init-hadoop-spark-master.sh", "active"]
    ports:
      - "9870:9870" # nn web-ui
      - "8088:8088" # yarn rm web-ui
      # - "9868:9868" # secondary nn web-ui (for single nn)

  hdp-master-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-2
    hostname: hdp-master-2
    command: ["init-hadoop-spark-master.sh", "standby"]
    ports:
      - "9871:9870" # nn web-ui
      - "8089:8088" # yarn rm web-ui

  hdp-master-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-3
    hostname: hdp-master-3
    command: ["init-hadoop-spark-manager.sh"]
    ports:
      - "19888:19888" # job history server web-ui

  hdp-worker-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-1
    hostname: hdp-worker-1
  hdp-worker-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-2
    hostname: hdp-worker-2
  hdp-worker-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-3
    hostname: hdp-worker-3


############################
##          Hive          ##
############################


x-hive-variables:
  common: &hive-common
    image: data-infra:hive
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        BUILD_HIVE: true

x-hive-cluster: &hive-cluster
  hive-1:
    <<: *hive-common
    container_name: hive-1
    hostname: hive-1
    command: ["init-hive.sh"]
    ports:
      # - "9083:9083" # metastore
      # - "10000:10000" # hiverserver2
      - "10002:10002" # hiveserver2 web-ui
    expose:
      - 9083 # metastore
      - 10000 # hiveserver2
    depends_on:
      mariadb: {"condition": "service_healthy"}


#############################
##          Kafka          ##
#############################


x-kafka-variables: 
  common: &kafka-common
    image: data-infra:kafka
    build:
      context: kafka
      dockerfile: dockerfile
    environment:
      ZOOKEEPER_CONNECT: zk-1:2181,zk-2:2181,zk-3:2181
    depends_on:
      zk-1: {"condition": "service_healthy"}
      zk-2: {"condition": "service_healthy"}
      zk-3: {"condition": "service_healthy"}
    expose:
      - 9092

x-kafka-cluster: &kafka-cluster
  kafka-1:
    <<: *kafka-common
    container_name: kafka-1
    hostname: kafka-1
  kafka-2:
    <<: *kafka-common
    container_name: kafka-2
    hostname: kafka-2
  kafka-3:
    <<: *kafka-common
    container_name: kafka-3
    hostname: kafka-3


################################
##          Services          ##
################################


services:
  <<: [
    *zookeeper-cluster,
    *hadoop-spark-cluster,
    *hive-cluster,
    *kafka-cluster,
  ]
  # Database
  mariadb:
    container_name: mariadb
    hostname: mariadb
    image: mariadb:11.1
    environment:
      MARIADB_ROOT_PASSWORD: root
    volumes:
      - ./mariadb/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: [ "CMD", "mariadb-admin", "-uroot", "-proot", "ping" ]
      interval: 5s
      retries: 5
    ports:
      - "3306:3306"


# networks:
#   default:
#     ipam:
#       config:
#         - subnet: 255.255.255.0
