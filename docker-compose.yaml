version: '3'


####################################
##          DB & Storage          ##
####################################


x-mariadb: &mariadb
  mariadb:
    container_name: mariadb
    hostname: mariadb
    image: mariadb:11.1
    environment:
      MARIADB_ROOT_PASSWORD: root
    volumes:
      - ./mariadb/init:/docker-entrypoint-initdb.d
    healthcheck:
      extra: [ "CMD", "mariadb-admin", "-uroot", "-proot", "ping" ]
      interval: 10s
      timeout: 50s
      retries: 10
    ports:
      - "3306:3306"

x-sqlserver: &sqlserver
  sqlserver:
    container_name: sqlserver
    hostname: sqlserver
    image: mcr.microsoft.com/mssql/server:2019-CU20-ubuntu-20.04
    environment:
      ACCEPT_EULA: Y
      MSSQL_SA_PASSWORD: SA_password! # ID: sa
      MSSQL_PID: Enterprise
    ports:
      - "1433:1433"

x-minio: &minio
  minio:
    container_name: minio
    hostname: minio
    image: quay.io/minio/minio:RELEASE.2024-01-31T20-20-33Z
    command: [ "server", "start", "--console-address", ":9001" ]
    environment:
      # https://min.io/docs/minio/linux/reference/minio-server/settings.html
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
      MINIO_VOLUMES: /data
    volumes:
      - "./minio/data:/data"
    ports:
      - "9000:9000"  # S3 API port
      - "9001:9001"  # web ui console
    # FIXME: image does not include curl
    # healthcheck:
    #   test: curl --fail --silent -f http://localhost:9000/minio/health/live
    #   interval: 10s
    #   timeout: 50s
    #   retries: 10


#################################
##          Zookeeper          ##
#################################


x-zookeeper-variables:
  common: &zookeeper-common
    image: zookeeper:3.8.3
    restart: on-failure
    healthcheck:
      test: [ "CMD", "zkServer.sh", "status" ]
      interval: 10s
      timeout: 50s
      retries: 10
  environment: &zookeeper-environment # https://hub.docker.com/_/zookeeper
    ZOO_PORT: 2181
    ZOO_SERVERS: >
      server.1=zk-1:2888:3888;2181
      server.2=zk-2:2888:3888;2181
      server.3=zk-3:2888:3888;2181
    ZOO_INIT_LIMIT: 10
    ZOO_SYNC_LIMIT: 5
    ZOO_TICK_TIME: 2000
    ZOO_DATA_DIR: /data

x-zookeeper-cluster: &zookeeper-cluster
  zk-1:
    <<: *zookeeper-common
    container_name: zk-1
    hostname: zk-1
    environment:
      ZOO_MY_ID: 1
      <<: *zookeeper-environment

  zk-2:
    <<: *zookeeper-common
    container_name: zk-2
    hostname: zk-2
    environment:
      ZOO_MY_ID: 2
      <<: *zookeeper-environment

  zk-3:
    <<: *zookeeper-common
    container_name: zk-3
    hostname: zk-3
    environment:
      ZOO_MY_ID: 3
      <<: *zookeeper-environment


####################################
##          Hadoop-Spark          ##
####################################


x-hadoop-spark-variables:
  common: &hadoop-spark-common
    image: data-infra:hadoop-spark
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        OPTIMIZE_HADOOP: true
        BUILD_SPARK: true
    environment:
      AWS_ACCESS_KEY_ID: minio
      AWS_SECRET_ACCESS_KEY: minio123
      AWS_ENDPOINT_URL: http://minio:9000
  master: &hadoop-spark-master
    depends_on:
      zk-1: {condition: service_healthy}
      zk-2: {condition: service_healthy}
      zk-3: {condition: service_healthy}
      hdp-worker-1: {condition: service_started}
      hdp-worker-2: {condition: service_started}
      hdp-worker-3: {condition: service_started}
  worker: &hadoop-spark-worker
    command: ["docker-entrypoint-hadoop-spark-worker.sh"]

x-hadoop-spark-cluster: &hadoop-spark-cluster
  hdp-master-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-1
    hostname: hdp-master-1
    command: ["docker-entrypoint-hadoop-spark-master.sh", "active"]
    ports:
      - "9870:9870" # nn web-ui
      - "8088:8088" # yarn rm web-ui
      # - "9868:9868" # secondary nn web-ui (for single nn)

  hdp-master-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-2
    hostname: hdp-master-2
    command: ["docker-entrypoint-hadoop-spark-master.sh", "standby"]
    ports:
      - "9871:9870" # nn web-ui
      - "8089:8088" # yarn rm web-ui

  hdp-master-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-3
    hostname: hdp-master-3
    command: ["docker-entrypoint-hadoop-spark-manager.sh"]
    ports:
      - "19888:19888" # job history server web-ui

  hdp-worker-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-1
    hostname: hdp-worker-1
  hdp-worker-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-2
    hostname: hdp-worker-2
  hdp-worker-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-3
    hostname: hdp-worker-3


############################
##          Hive          ##
############################


x-hive-variables:
  common: &hive-common
    image: data-infra:hive
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        BUILD_HIVE: true

x-hive-cluster: &hive-cluster
  hive-1:
    <<: *hive-common
    container_name: hive-1
    hostname: hive-1
    command: ["docker-entrypoint-hive.sh"]
    ports:
      # - "9083:9083" # metastore
      # - "10000:10000" # hiverserver2
      - "10002:10002" # hiveserver2 web-ui
    expose:
      - 9083 # metastore
      - 10000 # hiveserver2
    depends_on:
      mariadb: {condition: service_healthy, restart: true}


#############################
##          Kafka          ##
#############################


x-kafka-variables: 
  common: &kafka-common
    image: data-infra:kafka
    build:
      context: kafka
      dockerfile: dockerfile
    environment:
      ZOOKEEPER_CONNECT: zk-1:2181,zk-2:2181,zk-3:2181
    depends_on:
      zk-1: {condition: service_healthy}
      zk-2: {condition: service_healthy}
      zk-3: {condition: service_healthy}
    expose:
      - 9092

x-kafka-cluster: &kafka-cluster
  kafka-1:
    <<: *kafka-common
    container_name: kafka-1
    hostname: kafka-1
  kafka-2:
    <<: *kafka-common
    container_name: kafka-2
    hostname: kafka-2
  kafka-3:
    <<: *kafka-common
    container_name: kafka-3
    hostname: kafka-3


###############################
##          Elastic          ##
###############################


x-elastic-variable:
  init-elastic: &init-elastic
    container_name: init-elastic
    hostname: init-elastic
    image: data-infra:elasticsearch
    build:
      context: elastic/elasticsearch
      dockerfile: dockerfile
      args:
        VERSION: 8.11.0
    command: >
      bash -c '
      rm -f /tmp/ca.zip;
      bin/elasticsearch-certutil ca --silent --pem -out /tmp/ca.zip;
      unzip -o /tmp/ca.zip -d /etc/ssl/certs;
      rm -f /tmp/certs.zip;
      bin/elasticsearch-certutil cert --silent --pem -out /tmp/certs.zip \
        --in /etc/ssl/certs/instances.yml \
        --ca-cert /etc/ssl/certs/ca/ca.crt \
        --ca-key /etc/ssl/certs/ca/ca.key;
      unzip -o /tmp/certs.zip -d /etc/ssl/certs;
      echo 'All done!';
      '
    volumes:
      - ./elastic/common/certs:/etc/ssl/certs
  elasticsearch-common: &elasticsearch-common
    depends_on:
      init-elastic: {condition: service_completed_successfully}
    image: data-infra:elasticsearch
    build:
      context: elastic/elasticsearch
      dockerfile: dockerfile
      args:
        VERSION: 8.11.0
    ulimits:
      memlock:
        soft: -1
        hard: -1
    environment:
      # ES는 HEAP SIZE 30 ~ 32G가 최대치
      ES_JAVA_OPTS: -Xms1g -Xmx1g
      ELASTIC_PASSWORD: elastic
    volumes:
      - ./elastic/common/certs:/usr/share/elasticsearch/config/certs
      - ./elastic/elasticsearch/config/core/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      # - ./elastic/elasticsearch/shared:/mnt/shared
    healthcheck:
      test: '[ $(curl --silent --fail -u elasticadmin:elasticadmin http://localhost:9200/_nodes/$(hostname)/info --cacert ~/config/certs/ca/ca.crt | jq ._nodes.successful) -eq 1 ]'
      interval: 10s
      timeout: 50s
      retries: 10
  kibana-common: &kibana-common
    depends_on:
      init-elastic: {condition: service_completed_successfully}
      elasticsearch: {condition: service_healthy}
    image: data-infra:kibana
    build:
      context: elastic/kibana
      dockerfile: dockerfile
      args:
        VERSION: 8.11.0
    volumes:
      - ./elastic/common/certs:/usr/share/kibana/config/certs
      - ./elastic/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml
    healthcheck:
      test: '[ $(curl --silent --fail http://localhost:5601/api/status | jq .status.overall.level) = "\"available\"" ]'
      interval: 10s
      timeout: 50s
      retries: 10
  logstash-common: &logstash-common
    depends_on:
      init-elastic: {condition: service_completed_successfully}
      elasticsearch: {condition: service_healthy}
    image: docker.elastic.co/logstash/logstash:8.11.0
    volumes:
      - ./elastic/common/certs:/usr/share/logstash/config/certs
      - ./elastic/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      # - ./logstash/conf/logstash-filebeat.conf:/usr/share/logstash/config/logstash.conf

x-elastic-cluster: &elastic-cluster
  init-elastic:
    <<: *init-elastic
  elasticsearch:
    <<: *elasticsearch-common
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
  es-node-1:
    <<: *elasticsearch-common
    container_name: es-node-1
    hostname: es-node-1
    environment:
      - node.roles=master,data,ingest,transform,ml
  # es-node-2:
  #   <<: *elasticsearch-common
  #   container_name: es-node-2
  #   hostname: es-node-2
  #   environment:
  #     - node.roles=master,data
  # es-node-3:
  #   <<: *elasticsearch-common
  #   container_name: es-node-3
  #   hostname: es-node-3
  #   environment:
  #     - node.roles=master,data
  # es-node-4:
  #   <<: *elasticsearch-common
  #   container_name: es-node-4
  #   hostname: es-node-4
  #   environment:
  #     - node.roles=data,ingest
  # es-node-5:
  #   <<: *elasticsearch-common
  #   container_name: es-node-5
  #   hostname: es-node-5
  #   environment:
  #     - node.roles=data,ingest,transform

  # logstash-1:
  #   <<: *logstash-common
  #   container_name: logstash-1
  #   hostname: logstash-1
  #   ports:
  #     - "9600:9600"
  # logstash-2:
  #   <<: *logstash-common
  #   container_name: logstash-2
  #   hostname: logstash-2

  kibana:
    <<: *kibana-common
    container_name: kibana
    hostname: kibana
    ports:
      - "5601:5601"

  # elastic+     1     0  0 02:31 ?        00:00:00 /usr/bin/tini -- /usr/local/bin/docker-entrypoint sleep inf
  # elastic+     7     1  1 02:31 ?        00:00:01 elastic-agent container sleep inf
  fleet:
    # depends_on:
    #   init-elastic: {condition: service_completed_successfully}
    #   kibana: {condition: service_healthy}
    #   elasticsearch: {condition: service_healthy}
    container_name: fleet
    hostname: fleet
    image: docker.elastic.co/beats/elastic-agent:8.11.0
    command: sleep inf # FIXME
    # environment:
    #   FLEET_SERVER_ENABLE: true 
    #   FLEET_SERVER_ELASTICSEARCH_HOST: elasticsearch:9200 
    #   FLEET_SERVER_SERVICE_TOKEN: <service-token> 
    #   FLEET_SERVER_POLICY_ID: <fleet-server-policy> 
    ports:
      - 8220:8220 # fleet server
      - 8200:8200 # apm server


###########################
##          Ray          ##
###########################


x-ray-cluster: &ray-cluster


############################
##          Dask          ##
############################


x-dask-cluster: &dask-cluster


##################################
##          AI Servers          ##
##################################

# NOTE:
# fastapi는 ES VectorDB 기능을 테스트하기 위해서 생성하였으며, 나머지 서버는 추론 전용 프레임워크를 사용함
# kserve, seldon-core, cortex는 k8s 환경에서 사용가능한 프레임워크로써 여기서는 제외함

x-ai-servers: &ai-servers
  fastapi:
    container_name: fastapi
    hostname: fastapi
    image: data-infra:fastapi
    build:
      context: fastapi
      dockerfile: dockerfile
    command: sleep inf
    volumes:
      - ./fastapi/app:/opt/app
    ports:
      - "10000:8000"
    depends_on:
      elasticsearch: {condition: service_healthy, restart: true}
  # torchserve:
  #   container_name: torchserve
  #   hostname: torchserve
  #   image: data-infra:torchserve
  #   build:
  #     context: model-serving/torchserve
  #     dockerfile: dockerfile
  #   ports:
  #     - "10010:8080"  # inference server
  #     - "10011:8081"  # management server
  #     - "10012:8082"  # metrics server
  # bentoml:
  #   container_name: bentoml
  #   hostname: bentoml
  #   image: data-infra:bentoml
  #   build:
  #     context: model-serving/bentoml
  #     dockerfile: dockerfile
  #   ports:
  #     - "10020:3000"  # inference server
  triton:
    container_name: triton
    hostname: triton
    image: data-infra:triton
    build:
      context: model-serving/triton
      dockerfile: dockerfile
    ports:
      - "10030:8000" # http server
      - "10031:8001" # grpc server
      - "10032:8002" # metrics server
    healthcheck:
      test: curl --fail localhost:8000/v2/health/ready
      interval: 10s
      timeout: 50s
      retries: 10


##################################
##          Monitoring          ##
##################################


x-monitoring: &monitoring
  # grafana:
  # prometheus:
  # jabbix:


###############################
##          Jupyter          ##
###############################


x-jupyter: &jupyter
  jupyter:
    container_name: jupyter
    hostname: jupyter
    image: quay.io/jupyter/pytorch-notebook:pytorch-2.2.0
    # image: quay.io/jupyter/pyspark-notebook:spark-3.5.0
    environment:
      OPENAI_API_KEY: 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
      HUGGINGFACEHUB_API_TOKEN: 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
    volumes:
      - "./jupyter/data:/home/jovyan"
    ports:
      - "8888:8888"


##############################
##          Extras          ##
##############################


x-extras: &extras
  # TODO: Use as an production mode
  # airflow:
  #   container_name: airflow
  #   hostname: airflow
  #   image: data-infra:airflow
  #   build:
  #     context: airflow
  #     dockerfile: dockerfile
  #     args:
  #       AIRFLOW_VERSION: 2.6.3
  #       PYTHON_VERSION: 3.8
  #   volumes:
  #     - "./airflow/dags:/opt/airflow/dags"
  #     - "./airflow/plugins:/opt/airflow/plugins"
  #     - "./airflow/extensions:/opt/airflow/extensions"
  #   ports:
  #     - "8080:8080"
  #   depends_on:
  #     mairadb: {condition: service_healthy, restart: true}  # use an dependent database


####################################################
##          Definition of docker-compose          ##
####################################################


services:
  <<: [
    # *mariadb,
    # *sqlserver,
    # *minio,
    # *zookeeper-cluster,
    # *hadoop-spark-cluster,
    # *hive-cluster,
    # *kafka-cluster,
    *elastic-cluster,
    *ai-servers,
    *jupyter,
    # *extras,

    # *ray-cluster, # NOT IMPLEMENTED
    # *dask-cluster, # NOT IMPLEMENTED
    # *monitoring  # NOT IMPLEMENTED
  ]


networks:
  default:
    name: data-infra-on-docker
    driver: bridge
    # ipam:
    #   config:
    #     - subnet: 255.255.255.0
