version: '3'


#################################
##          Zookeeper          ##
#################################


x-zookeeper-variables:
  common: &zookeeper-common
    image: zookeeper:3.8.3
    restart: on-failure
    healthcheck:
      test: [ "CMD", "zkServer.sh", "status" ]
      interval: 10s
      retries: 5
  environment: &zookeeper-environment # https://hub.docker.com/_/zookeeper
    ZOO_PORT: 2181
    ZOO_SERVERS: >
      server.1=zk-1:2888:3888;2181
      server.2=zk-2:2888:3888;2181
      server.3=zk-3:2888:3888;2181
    ZOO_INIT_LIMIT: 10
    ZOO_SYNC_LIMIT: 5
    ZOO_TICK_TIME: 2000
    ZOO_DATA_DIR: /data

x-zookeeper-cluster: &zookeeper-cluster
  zk-1:
    <<: *zookeeper-common
    container_name: zk-1
    hostname: zk-1
    environment:
      ZOO_MY_ID: 1
      <<: *zookeeper-environment

  zk-2:
    <<: *zookeeper-common
    container_name: zk-2
    hostname: zk-2
    environment:
      ZOO_MY_ID: 2
      <<: *zookeeper-environment

  zk-3:
    <<: *zookeeper-common
    container_name: zk-3
    hostname: zk-3
    environment:
      ZOO_MY_ID: 3
      <<: *zookeeper-environment


####################################
##          Hadoop-Spark          ##
####################################


x-hadoop-spark-variables:
  common: &hadoop-spark-common
    image: data-infra:hadoop-spark
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        OPTIMIZE_HADOOP: true
        BUILD_SPARK: true
  master: &hadoop-spark-master
    depends_on:
      - zk-1
      - zk-2
      - zk-3
      - hdp-worker-1
      - hdp-worker-2
      - hdp-worker-3
  worker: &hadoop-spark-worker
    command: ["init-hadoop-spark-worker.sh"]

x-hadoop-spark-cluster: &hadoop-spark-cluster
  hdp-master-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-1
    hostname: hdp-master-1
    command: ["init-hadoop-spark-master.sh", "active"]
    ports:
      - "9870:9870" # nn web-ui
      - "8088:8088" # yarn rm web-ui
      # - "9868:9868" # secondary nn web-ui (for single nn)

  hdp-master-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-2
    hostname: hdp-master-2
    command: ["init-hadoop-spark-master.sh", "standby"]
    ports:
      - "9871:9870" # nn web-ui
      - "8089:8088" # yarn rm web-ui

  hdp-master-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-master ]
    container_name: hdp-master-3
    hostname: hdp-master-3
    command: ["init-hadoop-spark-manager.sh"]
    ports:
      - "19888:19888" # job history server web-ui

  hdp-worker-1:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-1
    hostname: hdp-worker-1
  hdp-worker-2:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-2
    hostname: hdp-worker-2
  hdp-worker-3:
    <<: [ *hadoop-spark-common, *hadoop-spark-worker ]
    container_name: hdp-worker-3
    hostname: hdp-worker-3


############################
##          Hive          ##
############################


x-hive-variables:
  common: &hive-common
    image: data-infra:hive
    build:
      context: hadoop
      dockerfile: dockerfile
      args:
        BUILD_HIVE: true

x-hive-cluster: &hive-cluster
  hive-1:
    <<: *hive-common
    container_name: hive-1
    hostname: hive-1
    command: ["init-hive.sh"]
    ports:
      # - "9083:9083" # metastore
      # - "10000:10000" # hiverserver2
      - "10002:10002" # hiveserver2 web-ui
    expose:
      - 9083 # metastore
      - 10000 # hiveserver2
    depends_on:
      mariadb: {"condition": "service_healthy"}


#############################
##          Kafka          ##
#############################


x-kafka-variables: 
  common: &kafka-common
    image: data-infra:kafka
    build:
      context: kafka
      dockerfile: dockerfile
    environment:
      ZOOKEEPER_CONNECT: zk-1:2181,zk-2:2181,zk-3:2181
    depends_on:
      zk-1: {"condition": "service_healthy"}
      zk-2: {"condition": "service_healthy"}
      zk-3: {"condition": "service_healthy"}
    expose:
      - 9092

x-kafka-cluster: &kafka-cluster
  kafka-1:
    <<: *kafka-common
    container_name: kafka-1
    hostname: kafka-1
  kafka-2:
    <<: *kafka-common
    container_name: kafka-2
    hostname: kafka-2
  kafka-3:
    <<: *kafka-common
    container_name: kafka-3
    hostname: kafka-3


###############################
##          Elastic          ##
###############################


x-elastic-variable:
  elasticsearch-common: &elasticsearch-common
    image: data-infra:elasticsearch
    build:
      context: elastic/elasticsearch
      dockerfile: dockerfile
      args:
        ES_VERSION: 8.11.3
    ulimits:
      memlock:
        soft: -1
        hard: -1
    environment:
      ES_JAVA_OPTS: -Xms1g -Xmx1g
      ELASTIC_PASSWORD: elastic
    volumes:
      - ./elastic/elasticsearch/config/core/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    #   - ./elastic/elasticsearch/shared:/mnt/shared
  kibana-common: &kibana-common
    image: docker.elastic.co/kibana/kibana:8.11.3
    volumes:
      - ./elastic/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml
    #   - ./elastic/kibana/shared:/mnt/shared
  logstash-common: &logstash-common
    image: docker.elastic.co/logstash/logstash:8.11.3
    volumes:
      - ./elastic/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    #   - ./logstash/conf/logstash-filebeat.conf:/usr/share/logstash/config/logstash.conf
    #   - ./logstash/shared:/mnt/shared

x-elastic-cluster: &elastic-cluster
  elasticsearch:
    <<: *elasticsearch-common
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
  es-node-1:
    <<: *elasticsearch-common
    container_name: es-node-1
    hostname: es-node-1
    environment:
      - node.roles=master,data,ingest,transform,ml
    ports:
      - "9201:9200"
  # es-node-2:
  #   <<: *elasticsearch-common
  #   container_name: es-node-2
  #   hostname: es-node-2
  #   environment:
  #     - node.roles=master,data
  # es-node-3:
  #   <<: *elasticsearch-common
  #   container_name: es-node-3
  #   hostname: es-node-3
  #   environment:
  #     - node.roles=master,data
  # es-node-4:
  #   <<: *elasticsearch-common
  #   container_name: es-node-4
  #   hostname: es-node-4
  #   environment:
  #     - node.roles=data,ingest
  # es-node-5:
  #   <<: *elasticsearch-common
  #   container_name: es-node-5
  #   hostname: es-node-5
  #   environment:
  #     - node.roles=data,ingest,transform

  # logstash-1:
  #   <<: *logstash-common
  #   container_name: logstash-1
  #   hostname: logstash-1
  #   ports:
  #     - "9600:9600"
  # logstash-2:
  #   <<: *logstash-common
  #   container_name: logstash-2
  #   hostname: logstash-2

  kibana:
    <<: *kibana-common
    container_name: kibana
    hostname: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch


################################
##          Services          ##
################################


services:
  <<: [
    # *zookeeper-cluster,
    # *hadoop-spark-cluster,
    # *hive-cluster,
    # *kafka-cluster,
    *elastic-cluster,
  ]
  # Database
  # mariadb:
  #   container_name: mariadb
  #   hostname: mariadb
  #   image: mariadb:11.1
  #   environment:
  #     MARIADB_ROOT_PASSWORD: root
  #   volumes:
  #     - ./mariadb/init:/docker-entrypoint-initdb.d
  #   healthcheck:
  #     test: [ "CMD", "mariadb-admin", "-uroot", "-proot", "ping" ]
  #     interval: 5s
  #     retries: 5
  #   ports:
  #     - "3306:3306"
  # sqlserver:
  #   container_name: sqlserver
  #   hostname: sqlserver
  #   image: mcr.microsoft.com/mssql/server:2019-CU20-ubuntu-20.04
  #   environment:
  #     ACCEPT_EULA: Y
  #     MSSQL_SA_PASSWORD: SA_password!
  #     MSSQL_PID: Enterprise
  #   ports:
  #     - "1433:1433"
  jupyter:
    container_name: jupyter
    hostname: jupyter
    image: jupyter/minimal-notebook:latest
    ports:
      - "8888:8888"


# networks:
#   default:
#     ipam:
#       config:
#         - subnet: 255.255.255.0
