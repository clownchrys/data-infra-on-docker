#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"

# Spark
spark.master                        yarn
# spark.driver.memory                 5g
# spark.yarn.am.memory                2g
# spark.executor.memory               16g
# spark.executor.instances            96
# spark.executor.cores                4
# spark.eventLog.enabled              true
spark.driver.memory                 1g
spark.yarn.am.memory                1g
spark.executor.memory               1g
spark.executor.instances            1
spark.driver.cores                  1
spark.executor.cores                1
spark.eventLog.enabled              false
spark.eventLog.dir                  hdfs:///spark-logs
spark.history.fs.logDirectory       hdfs:///spark-logs
spark.history.fs.cleaner.enabled    true
spark.history.fs.cleaner.interval   3d
spark.history.fs.cleaner.maxAge     7d
spark.yarn.historyServer.address    hdp-master-3:18080
spark.history.ui.port               18080
# spark.executor.memoryOverhead       2048
# spark.yarn.executor.memoryOverheadFactor 0.1875

# spark.jars.packages                 software.amazon.awssdk:s3:2.17.52,org.apache.hadoop:hadoop-aws:3.1.2

# SQL
spark.sql.extensions                    io.delta.sql.DeltaSparkSessionExtension
spark.sql.warehouse.dir                 /user/hive/warehouse
spark.sql.catalogImplementation         hive

# Catalog: default (spark_catalog)
spark.sql.catalog.spark_catalog         org.apache.spark.sql.delta.catalog.DeltaCatalog

# Catalog: additionally
# spark.sql.catalog.catalog1              org.apache.spark.sql.delta.catalog.DeltaCatalog
# spark.sql.catalog.catalog1.type         hive
# spark.sql.catalog.catalog1.warehouse    hdfs://hdp-cluster/user/hive/warehouse/catalog1

# DynamicAlloc
# spark.dynamicAllocation.enabled                 true
# spark.dynamicAllocation.initialExecutors        2
# spark.dynamicAllocation.minExecutors            2
# spark.dynamicAllocation.shuffleTrackingEnabled  true
# # spark.shuffle.service.enabled                   true # for spark-3.1.2

# Console
spark.ui.showConsolProgress         false

# FS: S3
# spark.hadoop.fs.s3.impl                     org.apache.hadoop.fs.s3native.NativeS3FileSystem
# spark.hadoop.fs.s3.credentials.provider     com.amazonaws.auth.ContainerCredentialsProvider
spark.hadoop.fs.s3.impl                     org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3.credentials.provider     org.apache.hadoop.fs.s3a.SimpleAWSCredentialProvider
spark.hadoop.fs.s3.access.key               minio
spark.hadoop.fs.s3.secret.key               minio123
spark.hadoop.fs.s3.endpoint                 http://minio:9000
spark.hadoop.fs.s3.connection.ssl.enabled   false
spark.hadoop.fs.s3.path.style.access        true

# FS: S3A
spark.hadoop.fs.s3a.impl                    org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.credentials.provider    org.apache.hadoop.fs.s3a.SimpleAWSCredentialProvider
spark.hadoop.fs.s3a.access.key              minio
spark.hadoop.fs.s3a.secret.key              minio123
spark.hadoop.fs.s3a.endpoint                http://minio:9000
spark.hadoop.fs.s3a.connection.ssl.enabled  false
spark.hadoop.fs.s3a.path.style.access       true