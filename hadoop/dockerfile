ARG SERVICE_ACCOUNT=hdp-user

ARG BUILD_SPARK=false
ARG BUILD_HIVE=false


# Service Account
FROM data-infra:os AS build-service-account
ARG SERVICE_ACCOUNT

RUN useradd ${SERVICE_ACCOUNT} --create-home --shell $(which bash) --group sudo
RUN echo "${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT}" | chpasswd
# RUN echo "${SERVICE_ACCOUNT} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN echo "${SERVICE_ACCOUNT} ALL=NOPASSWD:/usr/sbin/sshd" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN cp -rvf ~/.ssh /home/${SERVICE_ACCOUNT}/ && \
    chown -hR ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /home/${SERVICE_ACCOUNT}/.ssh


# Build Hadoop
FROM build-service-account AS build-hadoop
ARG HADOOP_VERSION=3.3.6

COPY ./resources/hadoop-${HADOOP_VERSION}.tar.gz /tmp
RUN mkdir /opt/hadoop
RUN tar xvf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /opt/hadoop 
RUN ln -sf /opt/hadoop/hadoop-${HADOOP_VERSION} /opt/hadoop/current

ENV HADOOP_HOME=/opt/hadoop/current
ENV HADOOP_MAPRED_HOME=/opt/hadoop/current
ENV HADOOP_COMMON_HOME=/opt/hadoop/current
ENV HADOOP_HDFS_HOME=/opt/hadoop/current
ENV HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
ENV YARN_HOME=${HADOOP_HOME}
ENV YARN_CONF_DIR=${HADOOP_CONF_DIR}
ENV PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}

COPY ./core/ha/config/* ${HADOOP_CONF_DIR}/
COPY ./core/ha/scripts/* /usr/sbin

ARG SERVICE_ACCOUNT
RUN mkdir -p \
    /data/hadoop/journalnode \
    /data/hadoop/datanode \
    /data/hadoop/namenode
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} \
    /opt/hadoop \
    /data/hadoop


# Build Spark
FROM build-hadoop AS build-spark-false
FROM build-hadoop AS build-spark-true
ARG SPARK_VERSION=3.4.2

COPY ./resources/spark-${SPARK_VERSION}-bin-hadoop3.tgz /tmp
RUN mkdir /opt/spark
RUN tar xvf /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/spark 
RUN ln -sf /opt/spark/spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark/current

RUN apt update && apt install -y python3-pip
RUN python3 -m pip install virtualenv
RUN virtualenv -p python3.8 /data/spark/venv
RUN /data/spark/venv/bin/pip3 install \
    findspark \
    pyhive \
    delta-spark

ENV SPARK_HOME=/opt/spark/current
ENV SPARK_CONF_DIR=${SPARK_HOME}/conf
ENV PYSPARK_PYTHON=/data/spark/venv/bin/python
ENV PYSPARK_DRIVER_PYTHON=/data/spark/venv/bin/python
ENV PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}

COPY ./resources/delta-core_2.12-2.4.0.jar ${SPARK_HOME}/jars/
COPY ./resources/delta-storage-2.4.0.jar ${SPARK_HOME}/jars/
COPY ./ecosystems/spark/config/* ${SPARK_CONF_DIR}/
COPY ./ecosystems/hive/config/hive-site.xml ${SPARK_CONF_DIR}/

ARG SERVICE_ACCOUNT
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /opt/spark
FROM build-spark-${BUILD_SPARK} as build-spark


# Build Hive
FROM build-spark AS build-hive-false
FROM build-spark AS build-hive-true
ARG HIVE_VERSION=3.1.3

COPY ./resources/apache-hive-${HIVE_VERSION}-bin.tar.gz /tmp
RUN mkdir /opt/hive
RUN tar xvf /tmp/apache-hive-${HIVE_VERSION}-bin.tar.gz -C /opt/hive
RUN ln -sf /opt/hive/apache-hive-${HIVE_VERSION}-bin /opt/hive/current

ENV HIVE_HOME=/opt/hive/current
ENV HIVE_CONF_DIR=${HIVE_HOME}/conf
ENV PATH=${HIVE_HOME}/bin:${HIVE_HOME}/sbin:${PATH}

COPY ./resources/mysql-connector-java-5.1.46.jar ${HIVE_HOME}/lib
COPY ./resources/guava-27.0-jre.jar ${HIVE_HOME}/lib
COPY ./ecosystems/hive/config/* ${HIVE_CONF_DIR}/
COPY ./ecosystems/hive/scripts/* /usr/sbin/

ARG SERVICE_ACCOUNT
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /opt/hive
FROM build-hive-${BUILD_HIVE} AS build-hive


# Runtime
FROM build-hive AS runtime

ARG SERVICE_ACCOUNT
USER ${SERVICE_ACCOUNT}
CMD [ "sleep", "inf" ]
