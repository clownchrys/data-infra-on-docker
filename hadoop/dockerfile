FROM data-infra:os

ARG HADOOP_VERSION=3.3.6
ARG SPARK_VERSION=3.4.2
ARG HIVE_VERSION=3.1.3

# 1. Install Hadoop
COPY ./resources/hadoop-${HADOOP_VERSION}.tar.gz /tmp
RUN mkdir /opt/hadoop
RUN tar xvf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /opt/hadoop 
RUN ln -sf /opt/hadoop/hadoop-${HADOOP_VERSION} /opt/hadoop/current

ENV HADOOP_HOME=/opt/hadoop/current
ENV HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
ENV HADOOP_PID_DIR=${HADOOP_HOME}/pids
ENV YARN_HOME=${HADOOP_HOME}
ENV YARN_CONF_DIR=${HADOOP_CONF_DIR}
ENV PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}

# 2. Install Spark
COPY ./resources/spark-${SPARK_VERSION}-bin-hadoop3.tgz /tmp
RUN mkdir /opt/spark
RUN tar xvf /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/spark 
RUN ln -sf /opt/spark/spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark/current

ENV SPARK_HOME=/opt/spark/current
ENV SPARK_CONF_DIR=${SPARK_HOME}/conf
ENV PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}

RUN apt update && apt install -y python3-pip
RUN python3 -m pip install virtualenv
RUN virtualenv -p python3.8 /data/spark/venv
RUN /data/spark/venv/bin/pip3 install \
    findspark \
    pyhive \
    delta-spark

COPY ./resources/delta-core_2.12-2.4.0.jar ${SPARK_HOME}/jars/
COPY ./resources/delta-storage-2.4.0.jar ${SPARK_HOME}/jars/

# 3. Install Hive
COPY ./resources/apache-hive-${HIVE_VERSION}-bin.tar.gz /tmp
RUN mkdir /opt/hive
RUN tar xvf /tmp/apache-hive-${HIVE_VERSION}-bin.tar.gz -C /opt/hive
RUN ln -sf /opt/hive/apache-hive-${HIVE_VERSION}-bin /opt/hive/current

ENV HIVE_HOME=/opt/hive/current
ENV HIVE_CONF_DIR=${HIVE_HOME}/conf
ENV PATH=${HIVE_HOME}/bin:${HIVE_HOME}/sbin:${PATH}

COPY ./resources/mysql-connector-java-5.1.46.jar ${HIVE_HOME}/lib
COPY ./resources/guava-27.0-jre.jar ${HIVE_HOME}/lib

# 4. Add Service User
ARG SERVICE_ACCOUNT=hdp-user
RUN useradd ${SERVICE_ACCOUNT} --create-home --shell $(which bash) --group sudo
RUN echo "${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT}" | chpasswd
# RUN echo "${SERVICE_ACCOUNT} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN echo "${SERVICE_ACCOUNT} ALL=NOPASSWD:/usr/sbin/sshd" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN cp -rvf ~/.ssh /home/${SERVICE_ACCOUNT}/ && \
    chown -hR ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /home/${SERVICE_ACCOUNT}/.ssh

RUN mkdir -p \
    /data/hadoop/journalnode \
    /data/hadoop/datanode \
    /data/hadoop/namenode
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} \
    /opt/hadoop \
    /opt/spark \
    /data/hadoop

# 5. On runtime
USER ${SERVICE_ACCOUNT}

COPY ./clusters/ha/config/hadoop/* ${HADOOP_CONF_DIR}/
COPY ./clusters/ha/config/spark/* ${SPARK_CONF_DIR}/
COPY ./clusters/ha/config/hive/hive-site.xml ${SPARK_CONF_DIR}/
COPY ./clusters/ha/config/hive/* ${HIVE_CONF_DIR}/

COPY ./clusters/ha/scripts/* /usr/sbin/
CMD [ "sleep", "inf" ]