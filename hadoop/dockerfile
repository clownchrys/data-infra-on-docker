ARG SERVICE_ACCOUNT=hdp-user

ARG OPTIMIZE_HADOOP=false
ARG BUILD_SPARK=false
ARG BUILD_HIVE=false


#######################################
##          Service Account          ##
#######################################


FROM data-infra:os AS build-service-account
ARG SERVICE_ACCOUNT

RUN useradd ${SERVICE_ACCOUNT} --create-home --shell $(which bash) --group sudo
RUN echo "${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT}" | chpasswd
# RUN echo "${SERVICE_ACCOUNT} ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN echo "${SERVICE_ACCOUNT} ALL=NOPASSWD:/usr/sbin/sshd" > /etc/sudoers.d/${SERVICE_ACCOUNT}
RUN cp -rvf ~/.ssh /home/${SERVICE_ACCOUNT}/ && \
    chown -hR ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /home/${SERVICE_ACCOUNT}/.ssh


##############################
##          Hadoop          ##
##############################


FROM build-service-account AS build-hadoop
ARG HADOOP_VERSION=3.3.6

COPY ./resources/hadoop-${HADOOP_VERSION}.tar.gz /tmp
RUN mkdir /opt/hadoop
RUN tar xvf /tmp/hadoop-${HADOOP_VERSION}.tar.gz -C /opt/hadoop 
RUN ln -sf /opt/hadoop/hadoop-${HADOOP_VERSION} /opt/hadoop/current

ENV HADOOP_HOME=/opt/hadoop/current
ENV HADOOP_MAPRED_HOME=/opt/hadoop/current
ENV HADOOP_COMMON_HOME=/opt/hadoop/current
ENV HADOOP_HDFS_HOME=/opt/hadoop/current
ENV HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
ENV YARN_HOME=${HADOOP_HOME}
ENV YARN_CONF_DIR=${HADOOP_CONF_DIR}
ENV PATH=${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${PATH}

COPY ./core/ha/config/* ${HADOOP_CONF_DIR}/
COPY ./core/ha/scripts/* /usr/sbin

ARG SERVICE_ACCOUNT
RUN mkdir -p \
    /data/hadoop/journalnode \
    /data/hadoop/datanode \
    /data/hadoop/namenode
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} \
    /opt/hadoop \
    /data/hadoop

FROM build-hadoop AS optimize-hadoop-false
FROM build-hadoop AS optimize-hadoop-true

# fstab (https://cloud-allstudy.tistory.com/719)
# [device] [mount point] [fs type] [opt1,opt2,...] [dump?] [file check?]
#RUN echo 'overlay /data overlay noatime,nodirtime 0 0' >> /etc/fstab
RUN echo 'overlay / overlay noatime,nodirtime 0 0' >> /etc/fstab

# limit.conf
RUN echo "${SERVICE_ACCOUNT} - nofile 32768" >> /etc/security/limit.conf

# sysctl
RUN echo 'vm.swappiness=1' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.core.somaxconn=1024' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'fs.file-max=6815744' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'fs.aio-max-nr=1048576' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.core.mem_default=262144' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.core.wmem_default=262144' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.core.mem_max=16777216' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.core.wmem_max=16777216' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.ipv4.tcp_mem=4096 262144 16777216' >> /etc/sysctl.d/20-hadoop.conf; \
    echo 'net.ipv4.tcp_wmem=4096 262144 16777216' >> /etc/sysctl.d/20-hadoop.conf;

FROM optimize-hadoop-${OPTIMIZE_HADOOP} AS optimize-hadoop


#############################
##          Spark          ##
#############################


FROM optimize-hadoop AS build-spark-false
FROM optimize-hadoop AS build-spark-true

ARG SPARK_VERSION=3.4.2

COPY ./resources/spark-${SPARK_VERSION}-bin-hadoop3.tgz /tmp
RUN mkdir /opt/spark
RUN tar xvf /tmp/spark-${SPARK_VERSION}-bin-hadoop3.tgz -C /opt/spark 
RUN ln -sf /opt/spark/spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark/current

RUN apt update && apt install -y python3-pip
RUN python3 -m pip install virtualenv
RUN virtualenv -p python3.8 /data/spark/venv
RUN /data/spark/venv/bin/pip3 install \
    findspark \
    pyhive \
    delta-spark

ENV SPARK_HOME=/opt/spark/current
ENV SPARK_CONF_DIR=${SPARK_HOME}/conf
ENV PYSPARK_PYTHON=/data/spark/venv/bin/python
ENV PYSPARK_DRIVER_PYTHON=/data/spark/venv/bin/python
ENV PATH=${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}

COPY ./resources/delta-core_2.12-2.4.0.jar ${SPARK_HOME}/jars/
COPY ./resources/delta-storage-2.4.0.jar ${SPARK_HOME}/jars/
COPY ./ecosystems/spark/config/* ${SPARK_CONF_DIR}/
COPY ./ecosystems/hive/config/hive-site.xml ${SPARK_CONF_DIR}/

ARG SERVICE_ACCOUNT
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /opt/spark

FROM build-spark-${BUILD_SPARK} as build-spark


############################
##          Hive          ##
############################


FROM build-spark AS build-hive-false
FROM build-spark AS build-hive-true

ARG HIVE_VERSION=3.1.3

COPY ./resources/apache-hive-${HIVE_VERSION}-bin.tar.gz /tmp
RUN mkdir /opt/hive
RUN tar xvf /tmp/apache-hive-${HIVE_VERSION}-bin.tar.gz -C /opt/hive
RUN ln -sf /opt/hive/apache-hive-${HIVE_VERSION}-bin /opt/hive/current

ENV HIVE_HOME=/opt/hive/current
ENV HIVE_CONF_DIR=${HIVE_HOME}/conf
ENV PATH=${HIVE_HOME}/bin:${HIVE_HOME}/sbin:${PATH}

COPY ./resources/mysql-connector-java-5.1.46.jar ${HIVE_HOME}/lib
COPY ./resources/guava-27.0-jre.jar ${HIVE_HOME}/lib
COPY ./ecosystems/hive/config/* ${HIVE_CONF_DIR}/
COPY ./ecosystems/hive/scripts/* /usr/sbin/

ARG SERVICE_ACCOUNT
RUN chown -R ${SERVICE_ACCOUNT}:${SERVICE_ACCOUNT} /opt/hive

FROM build-hive-${BUILD_HIVE} AS build-hive


###############################
##          Runtime          ##
###############################


FROM build-hive AS runtime

ARG SERVICE_ACCOUNT
USER ${SERVICE_ACCOUNT}
CMD [ "sleep", "inf" ]
