# https://pytorch.org/serve/configuration.html

# 5.3.3. Configure TorchServe listening address and port
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# 5.3.4. Configure TorchServe gRPC listening ports
grpc_inference_port=7070
grpc_management_port=7071

# 5.3.5. Enable SSL
# 1. Use a keystore:
#keystore=src/test/resources/keystore.p12
#keystore_pass=changeit
#keystore_type=PKCS12
# 2. Use private-key/certificate files:
#private_key_file=src/test/resources/key.pem
#certificate_file=src/test/resources/certs.pem

# 5.3.6. Configure Cross-Origin Resource Sharing (CORS)
# cors_allowed_origin is required to enable CORS, use '*' or your domain name
#cors_allowed_origin=https://yourdomain.com
# required if you want to use preflight request
#cors_allowed_methods=GET, POST, PUT, OPTIONS
# required if the request has an Access-Control-Request-Headers header
#cors_allowed_headers=X-Custom-Header

# 5.3.7. Prefer direct buffer
#prefer_direct_buffer=true

# 5.3.8. Allow model specific custom python packages
install_py_dep_per_model=true

# 5.3.9. Restrict backend worker to access environment variables
# a regular expression to filter out environment variable names. Default: all environment variables are visible to backend workers.
#blacklist_env_vars=

# 5.3.10. Limit GPU usage
# Maximum number of GPUs that TorchServe can use for inference. Default: all available GPUs in system.
#number_of_gpu=

# 5.3.11. Nvidia control Visibility
# Set nvidia environment variables. For example:
# export CUDA_DEVICE_ORDER="PCI_BUS_ID"
# export CUDA_VISIBLE_DEVICES="1,3"

# 5.3.12. Enable metrics api
# Enable or disable metric apis i.e. it can be either true or false. Default: true (Enabled)
enable_metrics_api=true
# Use this to specify metric report format.
# At present, the only supported and default value for this is prometheus
# This is used in conjunction with enable_metrics_api option above.
metrics_format=prometheus

# 5.3.13. Config model
# Use this to set configurations specific to a model. The value is presented in json format.
# A model's parameters are defined in model source code

# minWorkers: the minimum number of workers of a model
# maxWorkers: the maximum number of workers of a model
# batchSize: the batch size of a model
# maxBatchDelay: the maximum delay in msec of a batch of a model
# responseTimeout: the timeout in sec of a specific model?s response. This setting takes priority over default_response_timeout which is a default timeout over all models
# defaultVersion: the default version of a model
# marName: the mar file name of a model

#models={\
#  "noop": {\
#    "1.0": {\
#        "defaultVersion": true,\
#        "marName": "noop.mar",\
#        "minWorkers": 1,\
#        "maxWorkers": 1,\
#        "batchSize": 4,\
#        "maxBatchDelay": 100,\
#        "responseTimeout": 120\
#    }\
#  },\
#  "vgg16": {\
#    "1.0": {\
#        "defaultVersion": true,\
#        "marName": "vgg16.mar",\
#        "minWorkers": 1,\
#        "maxWorkers": 4,\
#        "batchSize": 8,\
#        "maxBatchDelay": 100,\
#        "responseTimeout": 120\
#    }\
#  }\
#}

# 5.3.14. Other properties
# Most of the following properties are designed for performance tuning.
# Adjusting these numbers will impact scalability and throughput.

# enable_envvars_config: Enable configuring TorchServe through environment variables. When this option is set to ?true?, all the static configurations of TorchServe can come through environment variables as well. Default: false
# number_of_netty_threads: Number frontend netty thread. This specifies the number of threads in the child EventLoopGroup of the frontend netty server. This group provides EventLoops for processing Netty Channel events (namely inference and management requests) from accepted connections. Default: number of logical processors available to the JVM. netty_client_threads: Number of backend netty thread. This specifies the number of threads in the WorkerThread EventLoopGroup which writes inference responses to the frontend. Default: number of logical processors available to the JVM.
# default_workers_per_model: Number of workers to create for each model that loaded at startup time. Default: available GPUs in system or number of logical processors available to the JVM.
# job_queue_size: Number inference jobs that frontend will queue before backend can serve. Default: 100.
# async_logging: Enable asynchronous logging for higher throughput, log output may be delayed if this is enabled. Default: false.
# default_response_timeout: Timeout, in seconds, used for all models backend workers before they are deemed unresponsive and rebooted. Default: 120 seconds.
# unregister_model_timeout: Timeout, in seconds, used when handling an unregister model request when cleaning a process before it is deemed unresponsive and an error response is sent. Default: 120 seconds.
# decode_input_request: Configuration to let backend workers to decode requests, when the content type is known. If this is set to ?true?, backend workers do ?Bytearray to JSON object? conversion when the content type is ?application/json? and the backend workers convert ?Bytearray to utf-8 string? when the Content-Type of the request is set to ?text*?. Default: true
# initial_worker_port : This is the initial port number for auto assigning port to worker process.
# model_store : Path of model store directory.
# model_server_home : Torchserve home directory.
# max_request_size : The maximum allowable request size that the Torchserve accepts, in bytes. Default: 6553500
# max_response_size : The maximum allowable response size that the Torchserve sends, in bytes. Default: 6553500
# limit_max_image_pixels : Default value is true (Use default PIL.Image.MAX_IMAGE_PIXELS). If this is set to ?false?, set PIL.Image.MAX_IMAGE_PIXELS = None in backend default vision handler for large image payload.
# allowed_urls : Comma separated regex of allowed source URL(s) from where models can be registered. Default: file://.*|http(s)?://.* (all URLs and local file system) e.g. : To allow base URLs https://s3.amazonaws.com/ and https://torchserve.pytorch.org/ use the following regex string allowed_urls=https://s3.amazonaws.com/.*,https://torchserve.pytorch.org/.*
# workflow_store : Path of workflow store directory. Defaults to model store directory.
# disable_system_metrics : Disable collection of system metrics when set to ?true?. Default value is ?false?.

# All the above config properties can be set using environment variable as follows.
# set enable_envvars_config to true in config.properties
# export environment variable for property asTS_<PROPERTY_NAME>.
# e.g.: to set inference_address property run cmd export TS_INFERENCE_ADDRESS="http://127.0.0.1:8082".

model_server_home=/opt/torchserve
model_store=/model-store
workflow_store=/model-store

enable_envvars_config=true
disable_system_metrics=true
async_logging=true

default_workers_per_model=1
default_response_timeout=60
job_queue_size=100000
decode_input_request=true